{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Lab 2: Data Preprocessing\n",
    "## Flood Prediction Dataset - Kaggle Playground Series S4E5\n",
    "\n",
    "**Dataset Description:**\n",
    "- **Target Variable:** FloodProbability\n",
    "- **Task:** Predict FloodProbability for test set\n",
    "- **Dataset:** Generated from deep learning model trained on Flood Prediction Factors dataset\n",
    "\n",
    "**Reference:** Following the preprocessing guide from Medium article (Part 1 & 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Import Required Libraries\n",
    "\n",
    "**Purpose:** Load all necessary libraries for data manipulation, visualization, and preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Set display options for better readability\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# Set plot style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"✓ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Load Dataset\n",
    "\n",
    "**Purpose:** Load the training data and perform initial size check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load training data\n# NOTE: Change the path based on your environment:\n\n# For Google Colab (with Google Drive mounted):\ndf = pd.read_csv('/content/drive/MyDrive/Dataset/playground-series-s4e5/train.csv')\n\n# For Local execution:\n# df = pd.read_csv('../data/train_original.csv')\n\nprint(\"Dataset loaded successfully!\")\nprint(f\"Shape: {df.shape}\")\nprint(f\"Rows: {df.shape[0]:,}\")\nprint(f\"Columns: {df.shape[1]}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Initial Data Exploration\n",
    "\n",
    "**Purpose:** Understand the dataset structure, data types, and basic statistics.\n",
    "\n",
    "### 3.1 Display First Few Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first 5 rows to understand data structure\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Dataset Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display dataset information: column names, data types, non-null counts\n",
    "print(\"Dataset Information:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Statistical Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary of numerical columns\n",
    "print(\"Statistical Summary:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Identify Column Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numerical and categorical columns\n",
    "numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(\"Column Type Analysis:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Numerical columns ({len(numerical_cols)}):\")\n",
    "print(numerical_cols)\n",
    "print(f\"\\nCategorical columns ({len(categorical_cols)}):\")\n",
    "print(categorical_cols if categorical_cols else \"None\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"OBSERVATION:\")\n",
    "print(\"All features are numerical (no categorical features found).\")\n",
    "print(\"Therefore, we will NOT need to apply encoding techniques.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Data Cleaning\n",
    "\n",
    "**Purpose:** Check for and handle missing values, duplicates, and outliers.\n",
    "\n",
    "### 4.1 Check for Missing Values\n",
    "\n",
    "**Reference Guide:** \"Check for missing values and apply imputation if needed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4.1: Check for missing values in each column\n",
    "print(\"STEP 4.1: Checking for Missing Values\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "missing_values = df.isna().sum()\n",
    "missing_percentage = (missing_values / len(df)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Column': missing_values.index,\n",
    "    'Missing_Count': missing_values.values,\n",
    "    'Percentage': missing_percentage.values\n",
    "})\n",
    "\n",
    "# Display only columns with missing values\n",
    "missing_data = missing_df[missing_df['Missing_Count'] > 0]\n",
    "\n",
    "if len(missing_data) > 0:\n",
    "    print(\"Columns with missing values:\")\n",
    "    print(missing_data)\n",
    "else:\n",
    "    print(\"✓ No missing values found in any column!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DECISION:\")\n",
    "if len(missing_data) > 0:\n",
    "    print(\"We WILL apply imputation using median (as per reference guide).\")\n",
    "else:\n",
    "    print(\"We will NOT apply any imputation technique.\")\n",
    "    print(\"REASON: No missing values detected in the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Check for Duplicate Rows\n",
    "\n",
    "**Reference Guide:** \"Remove duplicate rows and reset index.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4.2: Check for duplicate rows\n",
    "print(\"STEP 4.2: Checking for Duplicate Rows\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check duplicates (including ID column)\n",
    "duplicate_count_with_id = df.duplicated().sum()\n",
    "print(f\"Duplicate rows (with ID): {duplicate_count_with_id:,}\")\n",
    "\n",
    "# Check duplicates (excluding ID column - more meaningful)\n",
    "duplicate_count_no_id = df.drop('id', axis=1).duplicated().sum()\n",
    "print(f\"Duplicate rows (excluding ID): {duplicate_count_no_id:,}\")\n",
    "\n",
    "if duplicate_count_no_id > 0:\n",
    "    duplicate_percentage = (duplicate_count_no_id / len(df)) * 100\n",
    "    print(f\"Percentage of duplicates: {duplicate_percentage:.2f}%\")\n",
    "else:\n",
    "    print(\"✓ No duplicate rows found!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DECISION:\")\n",
    "if duplicate_count_no_id > 0:\n",
    "    print(f\"We WILL remove {duplicate_count_no_id:,} duplicate rows.\")\n",
    "    print(\"REASON: Duplicates can bias model training.\")\n",
    "    # Uncomment below to remove duplicates\n",
    "    # df = df.drop_duplicates().reset_index(drop=True)\n",
    "else:\n",
    "    print(\"We will NOT remove any rows.\")\n",
    "    print(\"REASON: No duplicates detected in the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Check for Outliers\n",
    "\n",
    "**Reference Guide:** \"Detect outliers using boxplots and IQR method, then decide whether to remove or replace based on domain knowledge.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4.3: Check for outliers - First, examine feature ranges\n",
    "print(\"STEP 4.3: Checking for Outliers\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get feature columns (exclude id and target)\n",
    "feature_cols = [col for col in df.columns if col not in ['id', 'FloodProbability']]\n",
    "\n",
    "print(f\"Analyzing {len(feature_cols)} features...\\n\")\n",
    "print(\"Feature Value Ranges:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Display range for each feature\n",
    "for col in feature_cols:\n",
    "    min_val = df[col].min()\n",
    "    max_val = df[col].max()\n",
    "    unique_vals = df[col].nunique()\n",
    "    print(f\"{col:40s} | Min: {min_val:2.0f} | Max: {max_val:2.0f} | Unique: {unique_vals:3d}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"OBSERVATION:\")\n",
    "print(\"All features are integer scores ranging from 0 to 18.\")\n",
    "print(\"These represent severity/rating scores for flood-related factors.\")\n",
    "print(\"Example: MonsoonIntensity=0 means 'low', MonsoonIntensity=16 means 'very high'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now check outliers using IQR method (as per reference guide)\n",
    "print(\"\\nOutlier Detection using IQR Method:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "outlier_summary = []\n",
    "\n",
    "for col in feature_cols:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    # Define outlier bounds\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Count outliers\n",
    "    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "    outlier_count = len(outliers)\n",
    "    outlier_pct = (outlier_count / len(df)) * 100\n",
    "    \n",
    "    if outlier_count > 0:\n",
    "        outlier_summary.append({\n",
    "            'Feature': col,\n",
    "            'Outliers': outlier_count,\n",
    "            'Percentage': outlier_pct\n",
    "        })\n",
    "\n",
    "if outlier_summary:\n",
    "    print(f\"\\nFeatures with outliers detected by IQR method:\")\n",
    "    print(\"-\" * 80)\n",
    "    # Show top 5 features with most outliers\n",
    "    for item in sorted(outlier_summary, key=lambda x: x['Percentage'], reverse=True)[:5]:\n",
    "        print(f\"{item['Feature']:40s} | {item['Outliers']:8,} outliers ({item['Percentage']:5.2f}%)\")\n",
    "    if len(outlier_summary) > 5:\n",
    "        print(f\"... and {len(outlier_summary) - 5} more features\")\n",
    "else:\n",
    "    print(\"✓ No outliers detected by IQR method.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DECISION:\")\n",
    "print(\"We will NOT remove any outliers.\")\n",
    "print(\"REASON:\")\n",
    "print(\"1. All values are within valid domain range (0-18 severity scores)\")\n",
    "print(\"2. High values (e.g., MonsoonIntensity=16) represent extreme conditions,\")\n",
    "print(\"   not data entry errors.\")\n",
    "print(\"3. As per reference guide: Use domain knowledge - these are valid observations.\")\n",
    "print(\"4. Removing extreme values would lose important information about\")\n",
    "print(\"   high-risk flood scenarios.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Correlation Analysis & Feature Selection\n",
    "\n",
    "**Purpose:** Analyze feature correlations to detect multicollinearity and select relevant features.\n",
    "\n",
    "**Reference Guide:** \"Remove features with correlation > 0.9 to avoid multicollinearity.\"\n",
    "\n",
    "### 5.1 Correlation with Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5.1: Calculate correlation with target variable\n",
    "print(\"STEP 5.1: Correlation with Target Variable\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get all columns except 'id'\n",
    "feature_cols_all = [col for col in df.columns if col != 'id']\n",
    "\n",
    "# Calculate correlation matrix\n",
    "corr_matrix = df[feature_cols_all].corr()\n",
    "\n",
    "# Get correlation with target\n",
    "target_corr = corr_matrix['FloodProbability'].sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nCorrelation of each feature with FloodProbability:\")\n",
    "print(\"-\" * 60)\n",
    "print(target_corr)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"OBSERVATION:\")\n",
    "print(f\"All features have weak-to-moderate correlation with target (0.17 to 0.19).\")\n",
    "print(f\"No feature has very strong correlation (>0.5).\")\n",
    "print(f\"This suggests all features contribute similarly to flood prediction.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Visualize Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5.2: Create correlation heatmap for visualization\n",
    "print(\"STEP 5.2: Creating Correlation Heatmap\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "plt.figure(figsize=(20, 16))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0, \n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Feature Correlation Matrix', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig('correlation_heatmap.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Correlation heatmap saved as 'correlation_heatmap.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Check for Multicollinearity\n",
    "\n",
    "**Reference Guide:** \"Remove highly correlated features (correlation > 0.9) to avoid multicollinearity.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5.3: Detect multicollinearity (highly correlated feature pairs)\n",
    "print(\"STEP 5.3: Checking for Multicollinearity\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Threshold: Correlation > 0.9\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Find highly correlated pairs (correlation > 0.9)\n",
    "high_corr_pairs = []\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i+1, len(corr_matrix.columns)):\n",
    "        if abs(corr_matrix.iloc[i, j]) > 0.9:\n",
    "            high_corr_pairs.append({\n",
    "                'Feature1': corr_matrix.columns[i],\n",
    "                'Feature2': corr_matrix.columns[j],\n",
    "                'Correlation': corr_matrix.iloc[i, j]\n",
    "            })\n",
    "\n",
    "if high_corr_pairs:\n",
    "    print(\"\\nHighly correlated feature pairs found:\")\n",
    "    for pair in high_corr_pairs:\n",
    "        print(f\"{pair['Feature1']:40s} <-> {pair['Feature2']:40s} | Corr: {pair['Correlation']:.4f}\")\n",
    "else:\n",
    "    print(\"✓ No highly correlated feature pairs found (correlation > 0.9)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DECISION:\")\n",
    "if high_corr_pairs:\n",
    "    print(f\"We WILL remove {len(high_corr_pairs)} feature(s) to avoid multicollinearity.\")\n",
    "    print(\"REASON: Highly correlated features provide redundant information.\")\n",
    "else:\n",
    "    print(\"We will NOT remove any features.\")\n",
    "    print(\"REASON:\")\n",
    "    print(\"1. No multicollinearity detected (all feature pairs < 0.9 correlation)\")\n",
    "    print(\"2. All features are independent and contribute unique information\")\n",
    "    print(\"3. All 20 features will be retained for modeling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Check if Encoding is Needed\n",
    "\n",
    "**Reference Guide:** \"Encode categorical features using One-Hot or Ordinal encoding.\"\n",
    "\n",
    "**Purpose:** Determine if categorical encoding is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Check if categorical encoding is needed\n",
    "print(\"STEP 6: Checking if Categorical Encoding is Needed\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(f\"Categorical columns found: {len(categorical_cols)}\")\n",
    "if categorical_cols:\n",
    "    print(\"Columns:\", categorical_cols)\n",
    "else:\n",
    "    print(\"None\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DECISION:\")\n",
    "if categorical_cols:\n",
    "    print(f\"We WILL apply encoding to {len(categorical_cols)} categorical feature(s).\")\n",
    "    print(\"Method: One-Hot Encoding for nominal, Ordinal Encoding for ordinal features.\")\n",
    "else:\n",
    "    print(\"We will NOT apply any encoding.\")\n",
    "    print(\"REASON: All features are already numerical (int64/float64).\")\n",
    "    print(\"No categorical features detected in the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: Check if Resampling is Needed\n",
    "\n",
    "**Reference Guide:** \"Handle imbalanced data using upsampling, downsampling, or SMOTE.\"\n",
    "\n",
    "**Purpose:** Determine if the dataset needs rebalancing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Check if resampling is needed for imbalanced data\n",
    "print(\"STEP 7: Checking if Resampling is Needed\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check if target is categorical (classification) or continuous (regression)\n",
    "target_dtype = df['FloodProbability'].dtype\n",
    "target_unique = df['FloodProbability'].nunique()\n",
    "target_min = df['FloodProbability'].min()\n",
    "target_max = df['FloodProbability'].max()\n",
    "\n",
    "print(f\"Target variable: FloodProbability\")\n",
    "print(f\"Data type: {target_dtype}\")\n",
    "print(f\"Unique values: {target_unique:,}\")\n",
    "print(f\"Range: {target_min:.3f} to {target_max:.3f}\")\n",
    "\n",
    "# Check if it's a classification or regression problem\n",
    "is_classification = target_unique < 50  # Arbitrary threshold\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"OBSERVATION:\")\n",
    "if is_classification:\n",
    "    print(\"This appears to be a CLASSIFICATION problem.\")\n",
    "    # Check class balance\n",
    "    class_dist = df['FloodProbability'].value_counts()\n",
    "    print(\"\\nClass distribution:\")\n",
    "    print(class_dist)\n",
    "else:\n",
    "    print(\"This is a REGRESSION problem (continuous target variable).\")\n",
    "    print(f\"Target has {target_unique:,} unique continuous values.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DECISION:\")\n",
    "if is_classification:\n",
    "    print(\"We MAY apply resampling techniques if classes are imbalanced.\")\n",
    "    print(\"Check class distribution above and apply SMOTE/upsampling if needed.\")\n",
    "else:\n",
    "    print(\"We will NOT apply any resampling techniques.\")\n",
    "    print(\"REASON:\")\n",
    "    print(\"1. This is a REGRESSION problem (predicting continuous values)\")\n",
    "    print(\"2. Resampling techniques (SMOTE, upsampling, downsampling) are\")\n",
    "    print(\"   designed for CLASSIFICATION problems with imbalanced classes\")\n",
    "    print(\"3. They are not applicable to regression tasks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8: Data Splitting\n",
    "\n",
    "**Reference Guide:** \"Split data into training (60-80%), validation (10-20%), and test (10-20%) sets.\"\n",
    "\n",
    "**Purpose:** Separate data before scaling to prevent data leakage.\n",
    "\n",
    "**Split Ratio:** 60% Train / 20% Validation / 20% Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Split data into train, validation, and test sets\n",
    "print(\"STEP 8: Data Splitting\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Split ratio: 60% Train / 20% Validation / 20% Test\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Separate features, target, and ID\n",
    "X = df.drop(['id', 'FloodProbability'], axis=1)  # Features only\n",
    "y = df['FloodProbability']                       # Target\n",
    "ids = df['id']                                   # IDs for reference\n",
    "\n",
    "print(f\"\\nOriginal dataset shape: {df.shape}\")\n",
    "print(f\"Features (X) shape: {X.shape}\")\n",
    "print(f\"Target (y) shape: {y.shape}\")\n",
    "\n",
    "# First split: 80% (train+valid) and 20% (test)\n",
    "X_temp, X_test, y_temp, y_test, ids_temp, ids_test = train_test_split(\n",
    "    X, y, ids, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Second split: 60% (train) and 20% (valid) from the 80%\n",
    "X_train, X_valid, y_train, y_valid, ids_train, ids_valid = train_test_split(\n",
    "    X_temp, y_temp, ids_temp, test_size=0.25, random_state=42  # 0.25 * 0.8 = 0.2\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Data Split Summary:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Training set:   {X_train.shape[0]:8,} samples ({X_train.shape[0]/len(df)*100:.1f}%)\")\n",
    "print(f\"Validation set: {X_valid.shape[0]:8,} samples ({X_valid.shape[0]/len(df)*100:.1f}%)\")\n",
    "print(f\"Test set:       {X_test.shape[0]:8,} samples ({X_test.shape[0]/len(df)*100:.1f}%)\")\n",
    "print(f\"Total:          {len(df):8,} samples (100.0%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"WHY SPLIT BEFORE SCALING?\")\n",
    "print(\"REASON:\")\n",
    "print(\"1. Prevents DATA LEAKAGE: If we scale before splitting, test data\")\n",
    "print(\"   statistics (mean, std) influence training data scaling.\")\n",
    "print(\"2. Correct approach: Fit scaler on TRAINING data only, then\")\n",
    "print(\"   transform validation and test sets using the same scaler.\")\n",
    "print(\"3. This ensures test set remains truly unseen during preprocessing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 9: Feature Scaling (Standardization)\n",
    "\n",
    "**Reference Guide:** \"Use StandardScaler for standardization (mean=0, std=1). Improves model performance.\"\n",
    "\n",
    "**Purpose:** Normalize features to standard scale for better model convergence.\n",
    "\n",
    "**Method:** StandardScaler (Mean=0, Std=1)\n",
    "\n",
    "### 9.1 Check Statistics Before Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9.1: Check statistics before scaling\n",
    "print(\"STEP 9.1: Statistics Before Scaling\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Training set feature statistics:\")\n",
    "print(\"-\" * 60)\n",
    "print(X_train.describe().loc[['mean', 'std']].round(2))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"OBSERVATION:\")\n",
    "print(\"Features have different scales (means around 4-5, std around 2).\")\n",
    "print(\"Scaling is needed for algorithms sensitive to feature magnitude.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Apply StandardScaler\n",
    "\n",
    "**Important:** Fit scaler on training data only!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9.2: Apply StandardScaler\n",
    "print(\"STEP 9.2: Applying StandardScaler\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Initialize StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit scaler on TRAINING data only\n",
    "print(\"Fitting scaler on training data...\")\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform validation and test data using the SAME scaler\n",
    "print(\"Transforming validation data...\")\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "print(\"Transforming test data...\")\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert back to DataFrames with column names\n",
    "feature_names = X_train.columns.tolist()\n",
    "\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=feature_names)\n",
    "X_valid_scaled_df = pd.DataFrame(X_valid_scaled, columns=feature_names)\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=feature_names)\n",
    "\n",
    "print(\"\\n✓ Scaling complete!\")\n",
    "\n",
    "# Verify scaling\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Statistics After Scaling:\")\n",
    "print(\"-\" * 60)\n",
    "print(X_train_scaled_df.describe().loc[['mean', 'std']].round(4))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"VERIFICATION:\")\n",
    "print(\"✓ Mean ≈ 0 for all features (standardized)\")\n",
    "print(\"✓ Std ≈ 1 for all features (standardized)\")\n",
    "print(\"✓ All features now on the same scale\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"WHY StandardScaler?\")\n",
    "print(\"REASON:\")\n",
    "print(\"1. Works well for algorithms assuming normal distribution\")\n",
    "print(\"   (Linear Regression, Logistic Regression, SVM, Neural Networks)\")\n",
    "print(\"2. Better for distance-based algorithms (KNN, K-means)\")\n",
    "print(\"3. Reference guide shows StandardScaler improves F1 and AUC-ROC scores\")\n",
    "print(\"4. Less sensitive to outliers compared to MinMaxScaler\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 10: Save Preprocessed Data\n",
    "\n",
    "**Purpose:** Save cleaned and scaled data for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 10: Reconstruct and save preprocessed datasets\nprint(\"STEP 10: Saving Preprocessed Data\")\nprint(\"=\" * 60)\n\n# Reconstruct full datasets with id and target\ntrain_preprocessed = pd.concat([\n    ids_train.reset_index(drop=True),\n    X_train_scaled_df.reset_index(drop=True),\n    y_train.reset_index(drop=True)\n], axis=1)\n\nvalid_preprocessed = pd.concat([\n    ids_valid.reset_index(drop=True),\n    X_valid_scaled_df.reset_index(drop=True),\n    y_valid.reset_index(drop=True)\n], axis=1)\n\ntest_preprocessed = pd.concat([\n    ids_test.reset_index(drop=True),\n    X_test_scaled_df.reset_index(drop=True),\n    y_test.reset_index(drop=True)\n], axis=1)\n\n# Save to CSV files in Google Drive\n# NOTE: Change these paths based on your environment:\n# - For Google Colab: Use '/content/drive/MyDrive/Dataset/playground-series-s4e5/'\n# - For Local: Use '../data/'\nprint(\"Saving files...\")\ntrain_preprocessed.to_csv('/content/drive/MyDrive/Dataset/playground-series-s4e5/train_preprocessed.csv', index=False)\nvalid_preprocessed.to_csv('/content/drive/MyDrive/Dataset/playground-series-s4e5/valid_preprocessed.csv', index=False)\ntest_preprocessed.to_csv('/content/drive/MyDrive/Dataset/playground-series-s4e5/test_preprocessed.csv', index=False)\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"Files Saved Successfully:\")\nprint(\"=\" * 60)\nprint(f\"✓ train_preprocessed.csv: {len(train_preprocessed):,} rows\")\nprint(f\"✓ valid_preprocessed.csv: {len(valid_preprocessed):,} rows\")\nprint(f\"✓ test_preprocessed.csv: {len(test_preprocessed):,} rows\")\n\nprint(\"\\nFiles saved to Google Drive!\")\nprint(\"Location: /content/drive/MyDrive/Dataset/playground-series-s4e5/\")\nprint(\"\\nThese files are ready for:\")\nprint(\"- Model training (train_preprocessed.csv)\")\nprint(\"- Hyperparameter tuning (valid_preprocessed.csv)\")\nprint(\"- Final evaluation (test_preprocessed.csv)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary: Preprocessing Decisions\n",
    "\n",
    "### Steps Taken and Justifications:\n",
    "\n",
    "| Step | Action Taken | Decision | Justification |\n",
    "|------|--------------|----------|---------------|\n",
    "| **1. Missing Values** | Checked | ✓ NO imputation applied | No missing values found (0/1,117,957 rows) |\n",
    "| **2. Duplicates** | Checked | ✓ NO removal needed | No duplicate rows found |\n",
    "| **3. Outliers** | Checked | ✓ NO removal applied | All values are valid domain scores (0-18 range) |\n",
    "| **4. Multicollinearity** | Checked | ✓ NO features removed | No correlation > 0.9 detected |\n",
    "| **5. Categorical Encoding** | Checked | ✓ NO encoding applied | All features already numerical |\n",
    "| **6. Resampling** | Checked | ✓ NO resampling applied | Regression problem (not classification) |\n",
    "| **7. Data Splitting** | Applied | ✓ 60/20/20 split | Standard practice for large datasets |\n",
    "| **8. Feature Scaling** | Applied | ✓ StandardScaler used | Best for regression, prevents data leakage |\n",
    "\n",
    "### Key Principles Followed:\n",
    "\n",
    "1. **Data-Driven Decisions:** Every decision based on actual data analysis, not assumptions\n",
    "2. **Prevented Data Leakage:** Split data BEFORE scaling\n",
    "3. **Domain Knowledge:** Kept \"outliers\" that represent valid extreme conditions\n",
    "4. **Best Practices:** Followed reference guide recommendations\n",
    "5. **No Unnecessary Steps:** Only applied preprocessing where data required it\n",
    "\n",
    "### Final Dataset Characteristics:\n",
    "\n",
    "- **Training Set:** 670,773 samples (60%)\n",
    "- **Validation Set:** 223,592 samples (20%)\n",
    "- **Test Set:** 223,592 samples (20%)\n",
    "- **Features:** 20 (all standardized, mean=0, std=1)\n",
    "- **Target:** FloodProbability (continuous, range: 0.285-0.725)\n",
    "- **Quality:** Clean, scaled, no leakage\n",
    "\n",
    "### Files Generated:\n",
    "\n",
    "1. `train_preprocessed.csv` - Ready for model training\n",
    "2. `valid_preprocessed.csv` - Ready for hyperparameter tuning\n",
    "3. `test_preprocessed.csv` - Ready for final evaluation\n",
    "4. `correlation_heatmap.png` - Feature correlation visualization\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. Model selection (Linear Regression, Random Forest, Gradient Boosting, etc.)\n",
    "2. Model training on `train_preprocessed.csv`\n",
    "3. Hyperparameter tuning using `valid_preprocessed.csv`\n",
    "4. Final evaluation on `test_preprocessed.csv`\n",
    "5. Metrics: MAE, RMSE, R² Score\n",
    "\n",
    "---\n",
    "\n",
    "**Preprocessing Complete! ✓**\n",
    "\n",
    "All decisions documented and justified according to the reference guide."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}